{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating SPECTER Embeddings for DHQ Corpus\n",
    "\n",
    "## Benjamin Charles Germain Lee\n",
    "\n",
    "This Jupyter notebook contains the code for generating SPECTER paper embeddings for the DHQ corpus using title + abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need this import for the huggingface implementation of SPECTER to work properly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# for tensor manipulation\n",
    "import torch\n",
    "\n",
    "# for distance calculation with embeddings\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# generic imports\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we load in the DHQ corpus data from a TSV (with titles & abstracts).\n",
    "\n",
    "This can be updated by swapping in a fresh TSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = []\n",
    "data = []\n",
    "\n",
    "row_ct = 0\n",
    "with open(\"2022-dhq-articles-with-abstracts.tsv\") as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "\n",
    "    for row in rd:\n",
    "        \n",
    "        if row_ct == 0:\n",
    "            fields = row\n",
    "        else:\n",
    "            data.append(row)\n",
    "        \n",
    "        row_ct += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Article ID', 'Pub. Year', 'Volume and Issue', 'Authors', 'Affiliations', 'Title', 'Abstract', '# of Cited Works']\n",
      "Number of papers: 643\n"
     ]
    }
   ],
   "source": [
    "# here are the headings:\n",
    "print(fields)\n",
    "print(\"Number of papers: \" + str(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, we sort the data by paper ID\n",
    "sorted_data = []\n",
    "for i in range(1, len(data)*10):\n",
    "    for row in data:\n",
    "        ID = int(row[0])\n",
    "        if ID == i:\n",
    "            sorted_data.append(row) \n",
    "            \n",
    "data = sorted_data\n",
    "\n",
    "# for i in range(0, len(data)):\n",
    "#     print(i, int(data[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, we produce the embeddings.\n",
    "\n",
    "Here, we feed in title + abstract (concatenated) to SPECTER. The code snippet here is modified from the SPECTER repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [00:36<00:00,  4.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "#load base model\n",
    "model = AutoModel.from_pretrained('allenai/specter2_base')\n",
    "\n",
    "# set batch size\n",
    "batch_size = 4\n",
    "\n",
    "# define chunking function for batches\n",
    "def chunk(file_list, n_chunks):\n",
    "    \n",
    "    # make chunks of files to be distributed across processes\n",
    "    chunks = []\n",
    "    chunk_size = math.ceil(float(len(file_list))/n_chunks)\n",
    "    for i in range(0, n_chunks-1):\n",
    "        chunks.append(file_list[i*chunk_size:(i+1)*chunk_size])\n",
    "    chunks.append(file_list[(n_chunks-1)*chunk_size:])\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# format papers for input\n",
    "papers = []\n",
    "for i in range(0, len(data)):\n",
    "    paper = {}\n",
    "    paper['ID'] = int(data[i][0])\n",
    "    paper['title'] = data[i][5]\n",
    "    paper['abstract'] = data[i][6]\n",
    "    papers.append(paper)\n",
    "\n",
    "# construct batches\n",
    "batches = chunk(papers, math.ceil(len(papers)/batch_size))\n",
    "\n",
    "embeddings_batches = []\n",
    "\n",
    "# generate embeddings\n",
    "for i in tqdm(range(0, len(batches))):\n",
    "    batch = batches[i]\n",
    "    # concatenate title and abstract\n",
    "    title_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in batch[:4]]\n",
    "    # preprocess the input\n",
    "    ### NOTE: CONSUMES A LOT OF MEMORY, LOWERED max_length TO 256 BUT THIS IS A TEMPRORARY FIX\n",
    "    ### SHOULD FIGURE OUT WHY MEMORY CONSUMPTION GROWS WITH EACH ITERATION...\n",
    "    inputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=\"pt\", max_length=300)\n",
    "    result = model(**inputs)\n",
    "    # take the first token in the batch as the embedding\n",
    "    embeddings_batches.append(result.last_hidden_state[:, 0, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we concatenate the embeddings (i.e., unravel batches)\n",
    "embeddings = embeddings_batches[0]\n",
    "for i in range(1, len(embeddings_batches)):\n",
    "    embeddings = torch.cat((embeddings, embeddings_batches[i]), 0)\n",
    "    \n",
    "# here, we assert that we have the right number of embeddings:\n",
    "assert len(papers) == len(embeddings)\n",
    "\n",
    "# convert from tensor to numpy array\n",
    "embeddings = embeddings.detach().numpy()\n",
    "\n",
    "# # now, we add the embeddings to the paper metadata\n",
    "# for i in range(0, len(papers)):\n",
    "#     papers[i]['embedding'] = embeddings[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, we can run tests on similar papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we generate pairwise distances\n",
    "## returned in this form: https://stackoverflow.com/questions/13079563/how-does-condensed-distance-matrix-work-pdist\n",
    "## i.e., a flattened array of the upper half of upper-triangular matrix\n",
    "condensed_distances = pdist(embeddings)\n",
    "# we expand to squareform matrix\n",
    "distances = squareform(condensed_distances)\n",
    "\n",
    "n_nearest_neighbors = 10\n",
    "\n",
    "nearest_neighbors = []\n",
    "for i in range(0, len(distances)):\n",
    "    # using trick from: https://stackoverflow.com/questions/34226400/find-the-index-of-the-k-smallest-values-of-a-numpy-array\n",
    "    # (could use argpartition if we need this fast, but the DHQ corpus is small enough that this is totally fine)\n",
    "    row_nearest_neighbor_indices = np.argsort(distances[i])[:n_nearest_neighbors+1]\n",
    "        \n",
    "    # here, we make sure the indexing is proper, recognizing that paper ID is not guaranteed to equal row number + 1\n",
    "    row_nearest_neighbors = []\n",
    "    for j in range(0, len(row_nearest_neighbor_indices)):\n",
    "        index = row_nearest_neighbor_indices[j]\n",
    "        row_nearest_neighbors.append(int(data[index][0]))\n",
    "            \n",
    "    nearest_neighbors.append(row_nearest_neighbors)\n",
    "\n",
    "# here, we populate papers but also make sure to slice off 0th nearest neighbor (itself)\n",
    "for i in range(0, len(papers)):\n",
    "    papers[i]['recommendations'] = nearest_neighbors[i][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lastly, we save the embeddings (as numpy array) and nearest neighbors (in the TSV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we save the embeddings\n",
    "with open('SPECTER_embeddings.npy', 'wb') as f:\n",
    "    np.save(f, embeddings)\n",
    "    \n",
    "# we also save an updated TSV where we list the paper IDs of the most similar papers\n",
    "\n",
    "# first, we update the fields\n",
    "new_fields = []\n",
    "for i in range(0, n_nearest_neighbors):\n",
    "    new_fields.append(\"Recommendation \" + str(i+1))\n",
    "del fields[-1]\n",
    "fields += new_fields\n",
    "\n",
    "# next, we update the data\n",
    "for i in range(0, len(data)):\n",
    "    del data[i][-1]\n",
    "    # we start at 1 to skip adding the paper itself as its own nearest neighbor\n",
    "    for j in range(1, len(nearest_neighbors[i])):\n",
    "        data[i].append(nearest_neighbors[i][j])\n",
    "\n",
    "out_data = [fields] + data\n",
    "\n",
    "\n",
    "with open(\"2022-dhq-articles-with-abstracts-and-SPECTER-recommendations.csv\", 'w') as csvfile: \n",
    "    csvwriter = csv.writer(csvfile) \n",
    "    for row in out_data:\n",
    "        csvwriter.writerow(row) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
